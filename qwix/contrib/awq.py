# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Integration of AWQ into Qwix.

AWQ (Activation-aware Weight Quantization) identifies salient weight channels
based on activation magnitudes and applies per-channel scaling to improve
quantization accuracy.

Usage:
1. Calibration: Use AwqCalibrationProvider to collect activation statistics.
2. Quantization: Use quantize_params to quantize weights with AWQ.
3. Inference: Use AwqInferenceProvider (not PtqProvider) to run the model.

The AWQ scales are stored separately from the quantized weights and applied
per-channel during inference for maximum accuracy.

Please check the test for an example usage.
"""

import dataclasses
from typing import Any

import flax
from flax import nnx
import jax
from qwix._src import averaging
from qwix._src import qconfig
from qwix._src.core import qarray
from qwix._src.providers import ptq
from qwix.contrib import awq_core
from qwix.contrib import calibration


@dataclasses.dataclass(frozen=True, kw_only=True)
class AwqRule(qconfig.QuantizationRule):
  """Use this rule to enable AWQ.

  Attributes:
    n_grid: Number of grid points for AWQ scale search. Default is 20.
  """

  n_grid: int = 20


@flax.struct.dataclass(kw_only=True)
class WithAwqScale(ptq.WithAux[qarray.QArray]):
  """A quantized array with AWQ per-channel scales.

  This wrapper stores the quantized weights along with the per-channel AWQ
  scales. During inference, the AwqInferenceProvider dequantizes the weights
  and divides by the AWQ scales to get the final weights.

  Attributes:
    awq_scale: Per-channel AWQ scales with shape (in_features,). This is a 1D
      array that will be broadcast along the contracting axis during inference.
    contracting_axis: The axis of the weight that is contracted in dot_general.
  """

  awq_scale: jax.Array
  contracting_axis: int = flax.struct.field(pytree_node=False)

# Register as NNX data to allow JAX arrays in Module attributes.
nnx.register_data_type(WithAwqScale)


class AwqCalibrationProvider(calibration.StatsCalibrationProvider):
  """Calibration provider for AWQ.

  This provider collects `awq_quant_stats` (per-channel activation scales) by
  using `StatsCalibrationProvider` to intercept compatible operations. These
  statistics are used by `quantize_params` to compute AWQ scales. This provider
  does not perform actual quantization or use quantized operations.
  """

  def get_rule_type(self) -> type[qconfig.QuantizationRule]:
    return AwqRule

  def compute_stats(self, lhs: jax.Array) -> dict[str, Any]:
    # Compute mean absolute activation for each channel (along sample axis).
    act_scale = awq_core.compute_act_scale(lhs, axis=1)
    return {'act_scale': act_scale}

  def get_stats_suffix(self) -> str:
    return '_awq'


def quantize_params(
    params: Any,
    abstract_quantized_params: Any,
    awq_quant_stats: Any,
    *,
    allow_extra_params: bool = False,
    n_grid: int = 20,
) -> Any:
  """Quantizes the params with AWQ.

  Args:
    params: The floating-point param tree to quantize.
    abstract_quantized_params: The param tree generated by the PTQ model,
      containing WithAux wrappers with HowToQuantize information.
    awq_quant_stats: The quant_stats dict from AwqCalibrationProvider. For
      params with no awq_quant_stats, they will be quantized with the default
      PTQ algorithm.
    allow_extra_params: If True, allow extra parameters not in
      abstract_quantized_params.
    n_grid: Number of grid points for AWQ scale search.

  Returns:
    The quantized params consumable by AwqInferenceProvider. For AWQ-quantized
    weights, returns WithAwqScale wrappers containing the QArray and per-channel
    AWQ scales. For non-AWQ weights, returns WithAux wrappers (same as PTQ).
  """
  quantized_params = {}
  not_quantized_params = {}
  for path, w in flax.traverse_util.flatten_dict(params).items():
    abs_w = ptq.get_value_from_path(abstract_quantized_params, path)
    awq_stats_path = (*path[:-1], path[-1] + '_awq')
    awq_stats = ptq.get_value_from_path(awq_quant_stats, awq_stats_path)

    if not isinstance(abs_w, ptq.WithAux) or awq_stats is None:
      # Not quantized by AWQ.
      not_quantized_params[path] = w
      continue

    # Get the contracting axis by assuming that all non-contracting axes
    # are in channelwise_axes.
    contracting_axis = set(range(w.ndim)) - set(abs_w.how.channelwise_axes)
    if len(contracting_axis) != 1:
      # Fallback to PTQ if we can't identify a single contracting axis.
      not_quantized_params[path] = w
      continue

    contracting_axis = list(contracting_axis)[0]

    # Normalize the weight to (ra, ca) format.
    w, restore_shape = awq_core.normalize_weight(w, contracting_axis)
    how = dataclasses.replace(abs_w.how, channelwise_axes=[0])
    if contracting_axis in how.tiled_axes:
      how = dataclasses.replace(
          how, tiled_axes={1: how.tiled_axes[contracting_axis]}
      )

    # Get the activation scale, which should be (ca,).
    calibration_stats = averaging.SimpleMovingAverage().get_calibration(
        awq_stats
    )
    activation_scale = calibration_stats['act_scale']
    assert activation_scale.shape[0] == w.shape[1]

    # Quantize the weight with AWQ.
    w_q, scales = awq_core.quantize_weight(
        w, activation_scale, how, n_grid=n_grid
    )

    # Restore original shape for QArray.
    w_q = restore_shape(w_q)

    # Store AWQ scales as 1D array (in_features,) for simplicity.
    # scales is (1, in_features), squeeze to (in_features,).
    awq_scale_1d = scales.squeeze(0)

    # Store AWQ scales separately for per-channel compensation during inference.
    quantized_params[path] = WithAwqScale(
        array=w_q,
        awq_scale=awq_scale_1d,
        contracting_axis=contracting_axis,
        how=abs_w.how,
    )

  # Quantize the non-AWQ params with PTQ.
  not_quantized_params = flax.traverse_util.unflatten_dict(not_quantized_params)
  ptq_quantized_params = ptq.quantize_params(
      not_quantized_params,
      abstract_quantized_params,
      allow_extra_params=allow_extra_params,
  )
  ptq_quantized_params = flax.traverse_util.flatten_dict(ptq_quantized_params)
  quantized_params.update(ptq_quantized_params)

  return flax.traverse_util.unflatten_dict(quantized_params)


class AwqInferenceProvider(ptq.PtqProvider):
  """Inference provider for AWQ.

  This provider handles both WithAwqScale (AWQ-quantized weights) and
  WithAux (PTQ-quantized weights).

  Unlike GPTQ, which produces standard QArray objects compatible with
  PtqProvider, AWQ requires this specialized provider to handle the on-the-fly
  scale compensation for weights wrapped in WithAwqScale.
  """

  def _apply_awq_scale(self, rhs: WithAwqScale) -> jax.Array:
    """Dequantizes and applies per-channel AWQ scale compensation."""
    rhs_dq = qarray.dequantize(rhs.array)

    # Reshape AWQ scales to broadcast along the contracting axis.
    # awq_scale is 1D (in_features,), need to add dims for broadcasting.
    scale_shape = [1] * rhs_dq.ndim
    scale_shape[rhs.contracting_axis] = rhs.awq_scale.shape[0]
    awq_scale_broadcast = rhs.awq_scale.reshape(scale_shape)

    return rhs_dq / awq_scale_broadcast

  def dot_general(
      self,
      lhs: jax.Array,
      rhs: jax.Array | WithAwqScale | ptq.WithAux[qarray.QArray],
      dimension_numbers: jax.lax.DotDimensionNumbers,
      precision: jax.lax.PrecisionLike = None,
      preferred_element_type: jax.typing.DTypeLike | None = None,
      *,
      out_sharding: jax.sharding.NamedSharding | None = None,
  ) -> jax.Array:
    # Handle AWQ-quantized weights with per-channel scale compensation.
    if isinstance(rhs, WithAwqScale):
      rhs = self._apply_awq_scale(rhs)

    # Use PtqProvider's dot_general for everything else (PTQ weights, etc.).
    return super().dot_general(
        lhs,
        rhs,
        dimension_numbers,
        precision=precision,
        preferred_element_type=preferred_element_type,
        out_sharding=out_sharding,
    )

  def einsum(
      self,
      einsum_str: str,
      *operands: jax.Array,
      precision: jax.lax.PrecisionLike = None,
      preferred_element_type: jax.typing.DTypeLike | None = None,
      _dot_general: Any = jax.lax.dot_general,
      out_sharding=None,
  ) -> jax.Array:
    def _preprocess_operand(op):
      if isinstance(op, WithAwqScale):
        return self._apply_awq_scale(op)
      return op

    new_operands = jax.tree.map(
        _preprocess_operand,
        operands,
        is_leaf=lambda x: isinstance(x, (WithAwqScale, ptq.WithAux)),
    )

    return super().einsum(
        einsum_str,
        *new_operands,
        precision=precision,
        preferred_element_type=preferred_element_type,
        _dot_general=_dot_general,
        out_sharding=out_sharding,
    )

  def get_intercept_map(self):
    """Used for interception."""
    return super().get_intercept_map() | {
        'jax.lax.dot_general': self.dot_general,
        'jax.numpy.einsum': self.einsum,
    }
